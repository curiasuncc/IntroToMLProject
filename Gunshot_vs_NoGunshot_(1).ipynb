{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AeZZ_cSsZfPx"},"source":["## Intalling dependencies\n","\n"]},{"cell_type":"code","metadata":{"id":"wbMc4vHjaYdQ"},"source":["! pip install tflite-model-maker"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z2ck_Ghdcgt9"},"source":["## Import TensorFlow, Model Maker and other libraries\n"]},{"cell_type":"code","metadata":{"id":"rwUA9u4oWoCR"},"source":["import tensorflow as tf\n","import tflite_model_maker as mm\n","from tflite_model_maker import audio_classifier\n","import os\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import itertools\n","import glob\n","import random\n","\n","from IPython.display import Audio, Image\n","from scipy.io import wavfile\n","\n","print(f\"TensorFlow Version: {tf.__version__}\")\n","print(f\"Model Maker Version: {mm.__version__}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIfm2TxKZAuA"},"source":["## The dataset\n","\n","Original dataset aquired from https://zenodo.org/record/3519845#.Y5U_nnbMKUl where audio files are WAV, Mono-Channel, 16 kHz, and 8-bit. Dataset was then converted to 16-bit to fit model training paramaters.Any new training data needs to be 16bit, WAV, Mono-Channel, 16 kHz.\n","\n","Description:\"The Sound Events for Surveillance Applications (SESA) dataset files were obtained from Freesound. The dataset was divided between train (480 files) and test (105 files) folders. All audio files are WAV, Mono-Channel, 16 kHz, and 8-bit with up to 33 seconds. # Classes: 0 - Casual (not a threat) 1 - Gunshot 2 - Explosion 3 - Siren (also contains alarms)\"\n","\n","The audios are already split in train and test folders. Inside each split folder, there's one folder for each bird, using their class_code as name.\n","\n","**If using google Colab, replicate 'dataset' folder hierarchy and upload audio files to respective folders to populate dataset in Colab."]},{"cell_type":"code","metadata":{"id":"ayd7UqCfQQFU"},"source":["# @title [Run this] Util functions and data structures. Will show gun picture if audio is gun, peace sign if audio is not a gun\n","\n","data_dir = './dataset'\n","\n","class_code_to_name = {\n","  'gun': 'GUN',\n","  'nogun': 'NOTGUN',   \n","}\n","#obtaining gun and peace sign images for confirming data\n","gun_images = {\n","  'gun': 'https://upload.wikimedia.org/wikipedia/commons/4/4f/SIG_Pro_by_Augustas_Didzgalvis.jpg', # \tAlejandro Bayer Tamayo from Armenia, Colombia \n","  'nogun': 'https://upload.wikimedia.org/wikipedia/commons/7/7b/Peace_%282462301168%29.jpg'\n","}\n","\n","test_files = os.path.join('/content', data_dir, 'test/*/*.wav')\n","\n","def get_random_audio_file():\n","  test_list = glob.glob(test_files)\n","  random_audio_path = random.choice(test_list)\n","  return random_audio_path\n","\n","\n","def show_class_data(audio_path):\n","  sample_rate, audio_data = wavfile.read(audio_path, 'rb')\n","\n","  class_code = audio_path.split('/')[-2]\n","  print(f'Class name: {class_code_to_name[class_code]}')\n","  print(f'Class code: {class_code}')\n","  display(Image(gun_images[class_code]))\n","\n","  plttitle = f'{class_code_to_name[class_code]} ({class_code})'\n","  plt.title(plttitle)\n","  plt.plot(audio_data)\n","  display(Audio(audio_data, rate=sample_rate))\n","\n","print('functions and data structures created')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yrv0uD7aXYl4"},"source":["### Playing some audio\n","\n","To have a better understanding about the data, lets listen to a random audio files from the test split.\n","\n","Note: later in this notebook you'll run inference on this audio for testing"]},{"cell_type":"code","metadata":{"id":"tEeMZh-VQy97"},"source":["random_audio = get_random_audio_file()\n","show_class_data(random_audio)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQj1Mf7YZELS"},"source":["## Training the Model\n","\n","[YAMNet](https://tfhub.dev/google/yamnet/1) is an audio event classifier trained on the AudioSet dataset to predict audio events from the AudioSet ontology.\n","\n","It's input is expected to be at 16kHz and with 1 channel.\n","\n","- `frame_length` is to decide how long each traininng sample is. in this caase EXPECTED_WAVEFORM_LENGTH * 3s\n","\n","- `frame_steps` is to decide how far appart are the training samples. In this case, the ith sample will start at EXPECTED_WAVEFORM_LENGTH * 6s after the (i-1)th sample.\n","\n","The reason to set these values is to work around some limitation in real world dataset.\n","\n","For example, in the bird dataset, birds don't sing all the time. They sing, rest and sing again, with noises in between. Having a long frame would help capture the singing, but setting it too long will reduce the number of samples for training.\n"]},{"cell_type":"code","metadata":{"id":"tUcxtfHXY7XS"},"source":["spec = audio_classifier.YamNetSpec(\n","    keep_yamnet_and_custom_heads=True,\n","    frame_step=1 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n","    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EF185yZ_M7zu"},"source":["## Loading the data\n","\n","Model Maker has the API to load the data from a folder and have it in the expected format for the model spec.\n","\n","The train and test split are based on the folders. The validation dataset will be created as 20% of the train split.\n","\n","Note: The `cache=True` is important to make training later faster but it will also require more RAM to hold the data. "]},{"cell_type":"code","metadata":{"id":"cX0RqETqZgzo"},"source":["train_data = audio_classifier.DataLoader.from_folder(\n","    spec, os.path.join(data_dir, 'train'), cache=True)\n","train_data, validation_data = train_data.split(0.8)\n","test_data = audio_classifier.DataLoader.from_folder(\n","    spec, os.path.join(data_dir, 'test'), cache=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziMghju-Rts2"},"source":["## Training the model\n","\n","the audio_classifier has the [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) method that creates a model and already start training it. \n","\n","You can customize many parameterss, for more information you can read more details in the documentation.\n","\n","On this first try you'll use all the default configurations and train for 100 epochs.\n","\n","Note: The first epoch takes longer than all the other ones because it's when the cache is created. After that each epoch takes close to 1 second."]},{"cell_type":"code","metadata":{"id":"8r6Awvl4ZkIv"},"source":["batch_size = 128\n","epochs = 100\n","\n","print('Training the model')\n","model = audio_classifier.create(\n","    train_data,\n","    spec,\n","    validation_data,\n","    batch_size=batch_size,\n","    epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXMEHZkAxJTl"},"source":[" run the evaluation step on the test data and vefify your model achieved good results on unseen data."]},{"cell_type":"code","metadata":{"id":"GDoQACMrZnOx"},"source":["print('Evaluating the model')\n","model.evaluate(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8QRRAM39aOxS"},"source":["## Understanding your model\n","\n","When training a classifier, it's useful to see the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The confusion matrix gives you detailed knowledge of how your classifier is performing on test data.\n"]},{"cell_type":"code","metadata":{"id":"zqB3c0368iH3"},"source":["def show_confusion_matrix(confusion, test_labels):\n","  \"\"\"Compute confusion matrix and normalize.\"\"\"\n","  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n","  axis_labels = test_labels\n","  ax = sns.heatmap(\n","      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n","      cmap='Blues', annot=True, fmt='.2f', square=True)\n","  plt.title(\"Confusion matrix\")\n","  plt.ylabel(\"True label\")\n","  plt.xlabel(\"Predicted label\")\n","\n","confusion_matrix = model.confusion_matrix(test_data)\n","show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gr1s7juBy7H"},"source":["## Testing the model [ONGOING]\n","\n","You can try the model on a sample audio from the test dataset just to see the results.\n","\n","First you get the serving model."]},{"cell_type":"code","metadata":{"id":"PmlmTl42Bq_u"},"source":["serving_model = model.create_serving_model()\n","\n","print(f'Model\\'s input shape and type: {serving_model.inputs}')\n","print(f'Model\\'s output shape and type: {serving_model.outputs}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQsZFO2mrYhx"},"source":["Coming back to the random audio you loaded earlier"]},{"cell_type":"code","metadata":{"id":"8dv5ViK0reXc"},"source":["# if you want to try another file just uncoment the line below\n","random_audio = get_random_audio_file()\n","show_class_data(random_audio)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uixOfKSUj_9m"},"source":["The model created has a fixed input window. \n","\n","For a given audio file, you'll have to split it in windows of data of the expected size. The last window might need to be filled with zeros."]},{"cell_type":"code","source":["print(serving_model.input_shape[1])"],"metadata":{"id":"2w0AyvHnvuNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAvGKQL0lNty"},"source":["sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n","\n","audio_data = np.array(audio_data) / tf.int16.max\n","input_size = serving_model.input_shape[1]\n","\n","splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n","\n","print(f'Test audio path: {random_audio}')\n","print(f'Original size of the audio data: {len(audio_data)}')\n","print(f'Number of windows for inference: {len(splitted_audio_data)}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLxKd0eFkMcR"},"source":["You'll loop over all the splitted audio and apply the model for each one of them.\n","\n","The model you've just trained has 2 outputs: The original YAMNet's output and the one you've just trained. This is important because the real world environment is more complicated than just bird sounds. You can use the YAMNet's output to filter out non relevant audio, for example, on the birds use case, if YAMNet is not classifying Birds or Animals, this might show that the output from your model might have an irrelevant classification.\n","\n","\n","Below both outpus are printed to make it easier to understand their relation. Most of the mistakes that your model make are when YAMNet's prediction is not related to your domain (eg: birds)."]},{"cell_type":"code","metadata":{"id":"4-8fJLrxGwYT"},"source":["print(random_audio)\n","\n","results = []\n","print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n","for i, data in enumerate(splitted_audio_data):\n","  yamnet_output, inference = serving_model(data)\n","  results.append(inference[0].numpy())\n","  result_index = tf.argmax(inference[0])\n","  spec_result_index = tf.argmax(yamnet_output[0])\n","  t = spec._yamnet_labels()[spec_result_index]\n","  result_str = f'Result of the window {i}: ' \\\n","  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n","  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n","  print(result_str)\n","\n","\n","results_np = np.array(results)\n","mean_results = results_np.mean(axis=0)\n","result_index = mean_results.argmax()\n","print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"],"execution_count":null,"outputs":[]}]}